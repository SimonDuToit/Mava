{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA backend failed to initialize: Unable to load cuPTI. Is it installed? (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state  # Useful dataclass to keep train state\n",
    "from typing import Any, Dict, Tuple\n",
    "from flax import struct\n",
    "import optax\n",
    "import jumanji\n",
    "import jumanji.wrappers\n",
    "import matplotlib.pyplot as plt\n",
    "import flashbax as fbx\n",
    "import flax\n",
    "import collections\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt\n",
    "import chex\n",
    "import neptune\n",
    "from jumanji.environments.routing.connector.generator import Generator, RandomWalkGenerator\n",
    "import hydra\n",
    "from mava.networks import FeedForwardActor as Actor\n",
    "from mava.networks import FeedForwardValueNet as Critic\n",
    "from jumanji.env import Environment\n",
    "from mava.systems.ppo.types import LearnerState, OptStates, Params, PPOTransition\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from mava.utils.training import make_learning_rate\n",
    "from mava.utils import make_env as environments\n",
    "from colorama import Fore, Style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([0, 1], dtype=uint32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[2441914641, 1384938218],\n",
       "       [3819641963, 2025898573]], dtype=uint32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.random.split(key.astype(jnp.float32).astype(jnp.uint32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach 1: Generate D with teacher, train on D with student.\n",
    "# Approach 2: Generate D0 with teacher. Repeat: train on D(0:n) with student, generate D(n+1) with student."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import pprint\n",
    "import time\n",
    "from optax import OptState\n",
    "from mava.evaluator import make_eval_fns\n",
    "from mava.types import ActorApply, LearnerFn\n",
    "from mava.utils.checkpointing import Checkpointer\n",
    "from mava.utils.jax import merge_leading_dims, unreplicate_batch_dim, unreplicate_n_dims\n",
    "from mava.utils.logger import LogEvent, MavaLogger\n",
    "from mava.utils.total_timestep_checker import check_total_timesteps\n",
    "from mava.wrappers.episode_metrics import get_final_step_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hydra import initialize, compose\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "with initialize(version_base=None, config_path=\"mava/configs\"):\n",
    "    config = compose(config_name='default_ff_ippo.yaml')\n",
    "    #print(OmegaConf.to_yaml(cfg))\n",
    "    OmegaConf.set_struct(config, False)\n",
    "\n",
    "key = jax.random.PRNGKey(config.system.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = (1 ,2) + (3, 4)\n",
    "a[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env, eval_env = environments.make(config)\n",
    "\n",
    "teacher_torso = hydra.utils.instantiate(config.network.actor_network.pre_torso)\n",
    "teacher_action_head = hydra.utils.instantiate(\n",
    "    config.network.action_head, action_dim=env.action_dim\n",
    ")\n",
    "actor_network = Actor(torso=teacher_torso, action_head=teacher_action_head)\n",
    "\n",
    "critic_torso = hydra.utils.instantiate(config.network.critic_network.pre_torso)\n",
    "critic_network = Critic(torso=critic_torso)\n",
    "\n",
    "teacher_policy = actor_network.apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Configured `CheckpointManager` using deprecated legacy API. Please follow the instructions at https://orbax.readthedocs.io/en/latest/api_refactor.html to migrate by May 1st, 2024.\n"
     ]
    }
   ],
   "source": [
    "obs = env.observation_spec().generate_value()\n",
    "init_x = jax.tree_util.tree_map(lambda x: x[jnp.newaxis, ...], obs)\n",
    "\n",
    "key, actor_net_key, critic_net_key = jax.random.split(key, 3)\n",
    "\n",
    "# Initialise actor params\n",
    "actor_params = actor_network.init(actor_net_key, init_x)\n",
    "critic_params = critic_network.init(critic_net_key, init_x)\n",
    "params = Params(actor_params, critic_params)\n",
    "\n",
    "loaded_checkpoint = Checkpointer(\n",
    "model_name=config.logger.system_name,\n",
    "**config.logger.checkpointing.load_args,  # Other checkpoint args\n",
    ")\n",
    "# Restore the learner state from the checkpoint\n",
    "restored_params, _ = loaded_checkpoint.restore_params(input_params=params)\n",
    "# Update the paramsp\n",
    "#teacher_params = restored_params\n",
    "teacher_params = restored_params.actor_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1m\u001b[32m\u001b[1mEVALUATOR - Steps per second: 67.721 | Episode length mean: 10.906 | Episode length std: 3.282 | Episode length min: 6.000 | Episode length max: 20.000 | Episode return mean: 0.103 | Episode return std: 1.234 | Episode return min: -1.350 | Episode return max: 2.520 | Winrate mean: 0.156 | Winrate std: 0.363 | Winrate min: 0.000 | Winrate max: 1.000\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "broadcast = lambda x: jnp.broadcast_to(x, (config.system.update_batch_size,) + x.shape)\n",
    "replicate_params = jax.tree_map(broadcast, teacher_params)\n",
    "\n",
    "# Duplicate learner across devices.\n",
    "replicate_params = flax.jax_utils.replicate(replicate_params, devices=jax.devices())\n",
    "\n",
    "evaluator, absolute_metric_evaluator = make_eval_fns(eval_env, actor_network, config)\n",
    "n_devices = 1\n",
    "logger = MavaLogger(config)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "trained_params = unreplicate_batch_dim(replicate_params)\n",
    "#trained_params = teacher_params\n",
    "key_e, *eval_keys = jax.random.split(key, n_devices + 1)\n",
    "eval_keys = jnp.stack(eval_keys)\n",
    "eval_keys = eval_keys.reshape(n_devices, -1)\n",
    "\n",
    "# Evaluate.\n",
    "evaluator_output = evaluator(trained_params, eval_keys)\n",
    "jax.block_until_ready(evaluator_output)\n",
    "\n",
    "# Log the results of the evaluation.\n",
    "elapsed_time = time.time() - start_time\n",
    "episode_return = jnp.mean(evaluator_output.episode_metrics[\"episode_return\"])\n",
    "\n",
    "steps_per_eval = int(jnp.sum(evaluator_output.episode_metrics[\"episode_length\"]))\n",
    "evaluator_output.episode_metrics[\"steps_per_second\"] = steps_per_eval / elapsed_time\n",
    "logger.log(evaluator_output.episode_metrics, 1,1, LogEvent.EVAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2, 800, 125)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "broadcast = lambda x: jnp.broadcast_to(x, (config.system.update_batch_size,) + x.shape)\n",
    "replicate_params = jax.tree_map(broadcast, teacher_params)\n",
    "replicate_params = flax.jax_utils.replicate(replicate_params, devices=jax.devices())\n",
    "replicate_params[\"params\"][\"action_head\"][\"Dense_0\"][\"kernel\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2, 16, 5, 5)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_devices = len(jax.devices())\n",
    "\n",
    "key, env_key = jax.random.split(key)\n",
    " # Initialise environment states and timesteps: across devices and batches.\n",
    "key, *env_keys = jax.random.split(\n",
    "    key, n_devices * config.system.update_batch_size * config.arch.num_envs + 1\n",
    "    #key, n_devices * config.system.update_batch_size + 1\n",
    ")\n",
    "env_states, timesteps = jax.vmap(env.reset, in_axes=(0))(\n",
    "    jnp.stack(env_keys),\n",
    ")\n",
    "reshape_states = lambda x: x.reshape(\n",
    "    (n_devices, config.system.update_batch_size, config.arch.num_envs) + x.shape[1:]\n",
    "    #(n_devices, config.system.update_batch_size) + x.shape[1:]\n",
    ")\n",
    "# (devices, update batch size, num_envs, ...)\n",
    "env_states = jax.tree_map(reshape_states, env_states)\n",
    "timesteps = jax.tree_map(reshape_states, timesteps)\n",
    "\n",
    "timesteps.observation.agents_view.shape\n",
    "env_states.env_state.grid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inner(x):\n",
    "    return x[0][0]\n",
    "\n",
    "def outer(x):\n",
    "    return x[0]\n",
    "\n",
    "def get_action_and_logits(params, obs):\n",
    "    output = teacher_policy(params, obs)\n",
    "    logits = output.distribution.logits\n",
    "    action = output.sample(seed=key)\n",
    "    return action, logits\n",
    "\n",
    "batched_get_action_and_logits = jax.pmap(jax.vmap(get_action_and_logits))\n",
    "action, logits = batched_get_action_and_logits(replicate_params, timesteps.observation);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(a, b):\n",
    "    return optax.softmax_cross_entropy_with_integer_labels(a, b).mean()\n",
    "new_params = replicate_params.copy()\n",
    "\n",
    "optim = optax.chain(\n",
    "        optax.clip_by_global_norm(config.system.max_grad_norm),\n",
    "        optax.adam(1.0e-1, eps=1e-5),\n",
    "    )\n",
    "opt_state = optim.init(new_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7222587e+36\n"
     ]
    }
   ],
   "source": [
    "\n",
    "action, logits = batched_get_action_and_logits(new_params, timesteps.observation);\n",
    "a = logits[0][0][0]\n",
    "#a = logits[0][0]\n",
    "b = jnp.zeros(a.shape,dtype=int)\n",
    "b = b.at[(0, -1)].set(1)\n",
    "b = b[0]\n",
    "#b = jax.nn.softmax(b)\n",
    "loss_info, grads = jax.value_and_grad(loss)(a,b)\n",
    "print(loss_info)\n",
    "updates, opt_state = optim.update(new_params, opt_state)\n",
    "new_params = optax.apply_updates(new_params, updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'params': {'action_head': {'Dense_0': {'bias': Array([[[-0.00101424,  0.0020198 ,  0.00586768, -0.00443786,\n",
       "              0.00316625, -0.00221972, -0.00344499,  0.00329099,\n",
       "             -0.00150719,  0.00699912,  0.00092079,  0.00146002,\n",
       "              0.00035564,  0.00514269, -0.00320692, -0.00227946,\n",
       "              0.00702267,  0.00147991, -0.00397861, -0.00429072,\n",
       "             -0.00410157, -0.00602203,  0.0025263 ,  0.00040068,\n",
       "             -0.00189015,  0.00214366,  0.00327747, -0.00459861,\n",
       "              0.00367538, -0.00329001,  0.00136478, -0.00363602,\n",
       "             -0.0058177 ,  0.00231216,  0.00321293, -0.00626311,\n",
       "              0.00141835,  0.0013156 , -0.00093854, -0.00471417,\n",
       "              0.00501029,  0.00073722, -0.00326477,  0.00365732,\n",
       "             -0.00630428, -0.00368691,  0.00436185,  0.00701323,\n",
       "              0.0053862 , -0.00217067, -0.00229137,  0.00637021,\n",
       "              0.00353427, -0.00278372, -0.00350193, -0.00110328,\n",
       "              0.00149027,  0.00044026, -0.00619619,  0.00363851,\n",
       "              0.00423801,  0.00107201,  0.00426515,  0.00220025,\n",
       "              0.00277849,  0.00069638,  0.00319404, -0.00107182,\n",
       "             -0.00445735,  0.00339313,  0.00147943, -0.0010674 ,\n",
       "              0.00046401, -0.00020693,  0.00064412,  0.00206264,\n",
       "              0.00695204, -0.00191685, -0.00344756,  0.00392132,\n",
       "              0.0031283 ,  0.00194907, -0.00030139, -0.00191286,\n",
       "              0.00364995,  0.00149429,  0.00200058, -0.00518607,\n",
       "              0.00025661,  0.00012585,  0.00318574,  0.0021424 ,\n",
       "              0.0020372 ,  0.00263067,  0.00329176, -0.00490885,\n",
       "              0.00661471, -0.00108562, -0.00337068, -0.00397543,\n",
       "             -0.0013889 ,  0.0025327 , -0.00284801, -0.00399016,\n",
       "              0.00074417, -0.00014119, -0.0020445 ,  0.00679868,\n",
       "             -0.00313038, -0.00378455, -0.00568536, -0.00615231,\n",
       "             -0.00144909, -0.00343992,  0.00189325, -0.00169123,\n",
       "              0.00537743,  0.00442366,  0.00681032, -0.00348404,\n",
       "              0.00018075,  0.00107809,  0.00097843,  0.00092519,\n",
       "              0.00219121],\n",
       "            [-0.00101424,  0.0020198 ,  0.00586768, -0.00443786,\n",
       "              0.00316625, -0.00221972, -0.00344499,  0.00329099,\n",
       "             -0.00150719,  0.00699912,  0.00092079,  0.00146002,\n",
       "              0.00035564,  0.00514269, -0.00320692, -0.00227946,\n",
       "              0.00702267,  0.00147991, -0.00397861, -0.00429072,\n",
       "             -0.00410157, -0.00602203,  0.0025263 ,  0.00040068,\n",
       "             -0.00189015,  0.00214366,  0.00327747, -0.00459861,\n",
       "              0.00367538, -0.00329001,  0.00136478, -0.00363602,\n",
       "             -0.0058177 ,  0.00231216,  0.00321293, -0.00626311,\n",
       "              0.00141835,  0.0013156 , -0.00093854, -0.00471417,\n",
       "              0.00501029,  0.00073722, -0.00326477,  0.00365732,\n",
       "             -0.00630428, -0.00368691,  0.00436185,  0.00701323,\n",
       "              0.0053862 , -0.00217067, -0.00229137,  0.00637021,\n",
       "              0.00353427, -0.00278372, -0.00350193, -0.00110328,\n",
       "              0.00149027,  0.00044026, -0.00619619,  0.00363851,\n",
       "              0.00423801,  0.00107201,  0.00426515,  0.00220025,\n",
       "              0.00277849,  0.00069638,  0.00319404, -0.00107182,\n",
       "             -0.00445735,  0.00339313,  0.00147943, -0.0010674 ,\n",
       "              0.00046401, -0.00020693,  0.00064412,  0.00206264,\n",
       "              0.00695204, -0.00191685, -0.00344756,  0.00392132,\n",
       "              0.0031283 ,  0.00194907, -0.00030139, -0.00191286,\n",
       "              0.00364995,  0.00149429,  0.00200058, -0.00518607,\n",
       "              0.00025661,  0.00012585,  0.00318574,  0.0021424 ,\n",
       "              0.0020372 ,  0.00263067,  0.00329176, -0.00490885,\n",
       "              0.00661471, -0.00108562, -0.00337068, -0.00397543,\n",
       "             -0.0013889 ,  0.0025327 , -0.00284801, -0.00399016,\n",
       "              0.00074417, -0.00014119, -0.0020445 ,  0.00679868,\n",
       "             -0.00313038, -0.00378455, -0.00568536, -0.00615231,\n",
       "             -0.00144909, -0.00343992,  0.00189325, -0.00169123,\n",
       "              0.00537743,  0.00442366,  0.00681032, -0.00348404,\n",
       "              0.00018075,  0.00107809,  0.00097843,  0.00092519,\n",
       "              0.00219121]]], dtype=float32),\n",
       "    'kernel': Array([[[[ 0.00238351,  0.00629629,  0.00590578, ..., -0.00319687,\n",
       "               0.00864987,  0.00014477],\n",
       "             [-0.00053658,  0.00410197,  0.00233144, ...,  0.00328572,\n",
       "              -0.00552045,  0.00221672],\n",
       "             [-0.00142891,  0.00561302,  0.00179061, ...,  0.00318935,\n",
       "               0.00377175,  0.00326005],\n",
       "             ...,\n",
       "             [-0.00610108, -0.00441915,  0.00328066, ..., -0.00226702,\n",
       "               0.00290553,  0.00042117],\n",
       "             [-0.00817224, -0.00411427,  0.00149552, ...,  0.00177891,\n",
       "              -0.0050707 , -0.00654997],\n",
       "             [-0.00703182, -0.00350919,  0.00191086, ..., -0.00085794,\n",
       "              -0.00673612, -0.00254902]],\n",
       "    \n",
       "            [[ 0.00238351,  0.00629629,  0.00590578, ..., -0.00319687,\n",
       "               0.00864987,  0.00014477],\n",
       "             [-0.00053658,  0.00410197,  0.00233144, ...,  0.00328572,\n",
       "              -0.00552045,  0.00221672],\n",
       "             [-0.00142891,  0.00561302,  0.00179061, ...,  0.00318935,\n",
       "               0.00377175,  0.00326005],\n",
       "             ...,\n",
       "             [-0.00610108, -0.00441915,  0.00328066, ..., -0.00226702,\n",
       "               0.00290553,  0.00042117],\n",
       "             [-0.00817224, -0.00411427,  0.00149552, ...,  0.00177891,\n",
       "              -0.0050707 , -0.00654997],\n",
       "             [-0.00703182, -0.00350919,  0.00191086, ..., -0.00085794,\n",
       "              -0.00673612, -0.00254902]]]], dtype=float32)}},\n",
       "  'torso': {'Conv_0': {'bias': Array([[[ 0.00354044, -0.0025378 ,  0.0036992 ,  0.00388744,\n",
       "             -0.00121242, -0.00242647, -0.00027973, -0.00234703,\n",
       "              0.00345098, -0.00242434,  0.00317041,  0.00256752,\n",
       "              0.002636  ,  0.00211058, -0.00059475,  0.00289469,\n",
       "              0.00285973, -0.00177188, -0.00135366,  0.00831109,\n",
       "              0.00235073, -0.00046117, -0.00152091,  0.00217024,\n",
       "             -0.00110684, -0.00158514,  0.00350863,  0.00388455,\n",
       "              0.00376493, -0.00228986,  0.00586801,  0.00398077],\n",
       "            [ 0.00354044, -0.0025378 ,  0.0036992 ,  0.00388744,\n",
       "             -0.00121242, -0.00242647, -0.00027973, -0.00234703,\n",
       "              0.00345098, -0.00242434,  0.00317041,  0.00256752,\n",
       "              0.002636  ,  0.00211058, -0.00059475,  0.00289469,\n",
       "              0.00285973, -0.00177188, -0.00135366,  0.00831109,\n",
       "              0.00235073, -0.00046117, -0.00152091,  0.00217024,\n",
       "             -0.00110684, -0.00158514,  0.00350863,  0.00388455,\n",
       "              0.00376493, -0.00228986,  0.00586801,  0.00398077]]],      dtype=float32),\n",
       "    'kernel': Array([[[[[[ 1.3443823e-03,  4.5423955e-03,  6.0606864e-03, ...,\n",
       "                 6.7544868e-03, -5.4399380e-03, -4.4568847e-03],\n",
       "               [-3.1143338e-03, -2.8489167e-03,  1.9880091e-03, ...,\n",
       "                -4.7313976e-03,  5.3032544e-03,  3.5033328e-03],\n",
       "               [ 5.5021700e-04,  4.9510919e-03, -1.6942462e-03, ...,\n",
       "                 1.3976698e-03, -3.6416845e-03, -2.0897812e-03]],\n",
       "    \n",
       "              [[ 5.8355499e-03,  2.3415773e-03, -4.9974476e-03, ...,\n",
       "                -3.2443195e-03, -5.4109227e-03, -2.3246519e-03],\n",
       "               [ 4.5282510e-03, -5.4771118e-03, -2.7529197e-03, ...,\n",
       "                 2.8264443e-03,  3.8988749e-03, -7.6178694e-04],\n",
       "               [-6.8612052e-03, -2.0735033e-03,  5.1319082e-03, ...,\n",
       "                -2.1385557e-03,  7.5227069e-03, -4.3413020e-03]],\n",
       "    \n",
       "              [[ 4.2992532e-03, -4.3619517e-03, -5.2906740e-03, ...,\n",
       "                -7.1933772e-04,  4.6652127e-03, -1.6715029e-03],\n",
       "               [ 4.0959166e-03, -1.3294095e-03, -1.8247787e-04, ...,\n",
       "                -4.6486002e-03, -4.9750684e-03, -2.8626495e-03],\n",
       "               [ 3.2064558e-03,  6.1145863e-03, -3.2040428e-03, ...,\n",
       "                -5.7449341e-03, -4.6987031e-03,  2.5792851e-03]]],\n",
       "    \n",
       "    \n",
       "             [[[ 8.3824005e-03,  2.4362935e-03, -2.2534574e-03, ...,\n",
       "                -3.2843081e-03, -3.6517077e-03,  8.3546648e-03],\n",
       "               [ 4.3622307e-03, -1.9242256e-03,  4.0309867e-03, ...,\n",
       "                -5.2710078e-03, -4.5221485e-04,  5.9425547e-03],\n",
       "               [ 4.0167687e-04, -6.1415355e-03,  5.0235423e-04, ...,\n",
       "                -4.1045966e-03, -2.0117501e-03, -5.4955678e-03]],\n",
       "    \n",
       "              [[-3.4283847e-04,  5.1882062e-03,  3.4609344e-03, ...,\n",
       "                -2.2443915e-03,  5.2367933e-03, -4.8822770e-03],\n",
       "               [-5.0375583e-03,  5.5419998e-03, -5.2693519e-03, ...,\n",
       "                 2.7673240e-03,  1.8771600e-03,  5.3193159e-03],\n",
       "               [-3.8263160e-03, -5.0071292e-03,  4.7710817e-03, ...,\n",
       "                -3.2207507e-03,  3.3137552e-04, -4.5261439e-04]],\n",
       "    \n",
       "              [[ 9.6391793e-04, -3.1581132e-03,  3.1040122e-03, ...,\n",
       "                -3.6129202e-03, -4.0940586e-03, -2.1886458e-03],\n",
       "               [-3.6779605e-03, -7.0369309e-03,  1.7554208e-03, ...,\n",
       "                -6.1584567e-04,  2.1647541e-03, -1.3833633e-03],\n",
       "               [ 4.7677574e-03, -5.4496378e-03, -1.1569860e-03, ...,\n",
       "                 7.1669254e-03,  6.7292764e-03,  4.0955036e-03]]],\n",
       "    \n",
       "    \n",
       "             [[[ 5.3139301e-03,  2.3881542e-03,  5.9950403e-03, ...,\n",
       "                 2.9688110e-03, -5.8202506e-03, -5.8274763e-04],\n",
       "               [-7.1655749e-03, -2.3338972e-03, -1.8420168e-03, ...,\n",
       "                -3.6971583e-03, -8.5045118e-05, -2.3134991e-03],\n",
       "               [-2.0662528e-03,  7.1492586e-03,  5.9177903e-03, ...,\n",
       "                -6.1176070e-03,  3.0447431e-03,  3.5557779e-03]],\n",
       "    \n",
       "              [[ 1.4433814e-03, -4.8893178e-04,  5.7076747e-03, ...,\n",
       "                -6.2217512e-03,  6.2223375e-03, -1.2937177e-04],\n",
       "               [ 4.7680200e-03, -4.5116525e-05, -4.1183122e-03, ...,\n",
       "                 1.7596749e-03,  2.2867916e-03, -1.2736814e-03],\n",
       "               [-4.0666871e-03,  5.0373393e-04,  2.8917862e-03, ...,\n",
       "                 2.7729939e-03,  4.5525385e-03, -1.1927702e-03]],\n",
       "    \n",
       "              [[-5.1832162e-03, -6.1353529e-03, -5.3792954e-03, ...,\n",
       "                -2.3994008e-03, -5.1038051e-03, -1.9060178e-03],\n",
       "               [ 4.7830679e-04,  4.4203037e-03, -8.3928136e-04, ...,\n",
       "                 2.8130328e-03, -6.6007175e-03, -1.9909202e-03],\n",
       "               [-4.4467929e-03,  4.2547532e-03,  2.5684629e-03, ...,\n",
       "                 6.9138650e-03,  4.8219645e-03, -6.4075133e-04]]]],\n",
       "    \n",
       "    \n",
       "    \n",
       "            [[[[ 1.3443823e-03,  4.5423955e-03,  6.0606864e-03, ...,\n",
       "                 6.7544868e-03, -5.4399380e-03, -4.4568847e-03],\n",
       "               [-3.1143338e-03, -2.8489167e-03,  1.9880091e-03, ...,\n",
       "                -4.7313976e-03,  5.3032544e-03,  3.5033328e-03],\n",
       "               [ 5.5021700e-04,  4.9510919e-03, -1.6942462e-03, ...,\n",
       "                 1.3976698e-03, -3.6416845e-03, -2.0897812e-03]],\n",
       "    \n",
       "              [[ 5.8355499e-03,  2.3415773e-03, -4.9974476e-03, ...,\n",
       "                -3.2443195e-03, -5.4109227e-03, -2.3246519e-03],\n",
       "               [ 4.5282510e-03, -5.4771118e-03, -2.7529197e-03, ...,\n",
       "                 2.8264443e-03,  3.8988749e-03, -7.6178694e-04],\n",
       "               [-6.8612052e-03, -2.0735033e-03,  5.1319082e-03, ...,\n",
       "                -2.1385557e-03,  7.5227069e-03, -4.3413020e-03]],\n",
       "    \n",
       "              [[ 4.2992532e-03, -4.3619517e-03, -5.2906740e-03, ...,\n",
       "                -7.1933772e-04,  4.6652127e-03, -1.6715029e-03],\n",
       "               [ 4.0959166e-03, -1.3294095e-03, -1.8247787e-04, ...,\n",
       "                -4.6486002e-03, -4.9750684e-03, -2.8626495e-03],\n",
       "               [ 3.2064558e-03,  6.1145863e-03, -3.2040428e-03, ...,\n",
       "                -5.7449341e-03, -4.6987031e-03,  2.5792851e-03]]],\n",
       "    \n",
       "    \n",
       "             [[[ 8.3824005e-03,  2.4362935e-03, -2.2534574e-03, ...,\n",
       "                -3.2843081e-03, -3.6517077e-03,  8.3546648e-03],\n",
       "               [ 4.3622307e-03, -1.9242256e-03,  4.0309867e-03, ...,\n",
       "                -5.2710078e-03, -4.5221485e-04,  5.9425547e-03],\n",
       "               [ 4.0167687e-04, -6.1415355e-03,  5.0235423e-04, ...,\n",
       "                -4.1045966e-03, -2.0117501e-03, -5.4955678e-03]],\n",
       "    \n",
       "              [[-3.4283847e-04,  5.1882062e-03,  3.4609344e-03, ...,\n",
       "                -2.2443915e-03,  5.2367933e-03, -4.8822770e-03],\n",
       "               [-5.0375583e-03,  5.5419998e-03, -5.2693519e-03, ...,\n",
       "                 2.7673240e-03,  1.8771600e-03,  5.3193159e-03],\n",
       "               [-3.8263160e-03, -5.0071292e-03,  4.7710817e-03, ...,\n",
       "                -3.2207507e-03,  3.3137552e-04, -4.5261439e-04]],\n",
       "    \n",
       "              [[ 9.6391793e-04, -3.1581132e-03,  3.1040122e-03, ...,\n",
       "                -3.6129202e-03, -4.0940586e-03, -2.1886458e-03],\n",
       "               [-3.6779605e-03, -7.0369309e-03,  1.7554208e-03, ...,\n",
       "                -6.1584567e-04,  2.1647541e-03, -1.3833633e-03],\n",
       "               [ 4.7677574e-03, -5.4496378e-03, -1.1569860e-03, ...,\n",
       "                 7.1669254e-03,  6.7292764e-03,  4.0955036e-03]]],\n",
       "    \n",
       "    \n",
       "             [[[ 5.3139301e-03,  2.3881542e-03,  5.9950403e-03, ...,\n",
       "                 2.9688110e-03, -5.8202506e-03, -5.8274763e-04],\n",
       "               [-7.1655749e-03, -2.3338972e-03, -1.8420168e-03, ...,\n",
       "                -3.6971583e-03, -8.5045118e-05, -2.3134991e-03],\n",
       "               [-2.0662528e-03,  7.1492586e-03,  5.9177903e-03, ...,\n",
       "                -6.1176070e-03,  3.0447431e-03,  3.5557779e-03]],\n",
       "    \n",
       "              [[ 1.4433814e-03, -4.8893178e-04,  5.7076747e-03, ...,\n",
       "                -6.2217512e-03,  6.2223375e-03, -1.2937177e-04],\n",
       "               [ 4.7680200e-03, -4.5116525e-05, -4.1183122e-03, ...,\n",
       "                 1.7596749e-03,  2.2867916e-03, -1.2736814e-03],\n",
       "               [-4.0666871e-03,  5.0373393e-04,  2.8917862e-03, ...,\n",
       "                 2.7729939e-03,  4.5525385e-03, -1.1927702e-03]],\n",
       "    \n",
       "              [[-5.1832162e-03, -6.1353529e-03, -5.3792954e-03, ...,\n",
       "                -2.3994008e-03, -5.1038051e-03, -1.9060178e-03],\n",
       "               [ 4.7830679e-04,  4.4203037e-03, -8.3928136e-04, ...,\n",
       "                 2.8130328e-03, -6.6007175e-03, -1.9909202e-03],\n",
       "               [-4.4467929e-03,  4.2547532e-03,  2.5684629e-03, ...,\n",
       "                 6.9138650e-03,  4.8219645e-03, -6.4075133e-04]]]]]],      dtype=float32)},\n",
       "   'Conv_1': {'bias': Array([[[-2.2940328e-03,  8.3989073e-03,  2.1231784e-03, -2.4397722e-03,\n",
       "             -1.6254373e-03,  3.8629887e-03,  8.7036286e-04,  3.8581975e-03,\n",
       "             -5.6236517e-05,  6.4285747e-03,  7.1263900e-03, -3.4665409e-03,\n",
       "             -4.5246608e-03, -1.5902352e-03,  3.8737191e-03, -2.0269980e-03,\n",
       "             -1.2754572e-03, -9.3422341e-04, -2.0341293e-03, -1.7018453e-03,\n",
       "             -2.4315640e-03,  3.8155750e-03,  8.1199296e-03,  3.1104069e-03,\n",
       "              3.2098873e-03,  7.6860003e-03,  3.6467791e-03,  8.4165484e-04,\n",
       "             -2.9687681e-03,  7.7194610e-04,  3.5744864e-03,  2.3178803e-03],\n",
       "            [-2.2940328e-03,  8.3989073e-03,  2.1231784e-03, -2.4397722e-03,\n",
       "             -1.6254373e-03,  3.8629887e-03,  8.7036286e-04,  3.8581975e-03,\n",
       "             -5.6236517e-05,  6.4285747e-03,  7.1263900e-03, -3.4665409e-03,\n",
       "             -4.5246608e-03, -1.5902352e-03,  3.8737191e-03, -2.0269980e-03,\n",
       "             -1.2754572e-03, -9.3422341e-04, -2.0341293e-03, -1.7018453e-03,\n",
       "             -2.4315640e-03,  3.8155750e-03,  8.1199296e-03,  3.1104069e-03,\n",
       "              3.2098873e-03,  7.6860003e-03,  3.6467791e-03,  8.4165484e-04,\n",
       "             -2.9687681e-03,  7.7194610e-04,  3.5744864e-03,  2.3178803e-03]]],      dtype=float32),\n",
       "    'kernel': Array([[[[[[-2.4747087e-03,  3.2835149e-03,  5.5959350e-03, ...,\n",
       "                 7.1633025e-03,  6.3672690e-03,  2.7601151e-03],\n",
       "               [-3.8429750e-03,  6.4895619e-03,  6.8725250e-04, ...,\n",
       "                -2.4378360e-03,  1.9542133e-03, -2.1264395e-03],\n",
       "               [-3.0517632e-03,  2.1946870e-03,  2.0760894e-03, ...,\n",
       "                -4.6644830e-03, -6.5434526e-04,  6.6185119e-03],\n",
       "               ...,\n",
       "               [ 1.5442676e-03,  6.7999563e-03,  1.0773402e-03, ...,\n",
       "                 1.5265064e-03,  2.4080132e-03,  1.9979239e-03],\n",
       "               [ 5.9022354e-03, -5.7591277e-04, -4.4386205e-04, ...,\n",
       "                -7.6640160e-03, -1.5586624e-03,  2.4048446e-03],\n",
       "               [-5.6537339e-03, -1.3616006e-03, -4.7771400e-03, ...,\n",
       "                 1.7859787e-03, -7.1272585e-03,  2.3960159e-03]],\n",
       "    \n",
       "              [[ 6.0675084e-04,  2.8819200e-03,  4.8605395e-03, ...,\n",
       "                -2.2791559e-03, -1.5065656e-03, -6.7509697e-03],\n",
       "               [-2.6977668e-03, -5.4235263e-03,  2.3007439e-04, ...,\n",
       "                 1.3120646e-03, -4.6037594e-03,  1.3771721e-03],\n",
       "               [ 1.7126997e-03, -7.1720192e-03,  8.7375240e-04, ...,\n",
       "                -7.1052006e-03, -7.1304603e-03, -1.1354764e-03],\n",
       "               ...,\n",
       "               [ 7.0849145e-03, -2.4828822e-03,  3.1305996e-03, ...,\n",
       "                 1.6291814e-03,  7.0163561e-04,  8.4064732e-04],\n",
       "               [ 2.2631690e-03, -1.1668573e-03, -1.1483632e-03, ...,\n",
       "                -2.7040141e-03, -6.1458079e-03,  7.9009915e-04],\n",
       "               [-4.0228624e-04, -2.6521306e-03, -7.7544176e-04, ...,\n",
       "                -6.6636540e-03,  1.2557907e-03, -5.1996754e-03]],\n",
       "    \n",
       "              [[ 3.6639944e-03, -3.3886351e-03,  2.5561901e-03, ...,\n",
       "                -7.7239756e-04,  5.7892012e-03,  1.6495874e-03],\n",
       "               [ 1.4946886e-03, -3.6531407e-04,  2.9049753e-03, ...,\n",
       "                -1.7827738e-03, -6.7403400e-03, -1.4600235e-03],\n",
       "               [-2.1766894e-04, -6.4839837e-03, -3.6291447e-03, ...,\n",
       "                 1.9590836e-03, -3.8586466e-03,  1.4126833e-03],\n",
       "               ...,\n",
       "               [-7.0356298e-03, -7.0897755e-03, -1.7116135e-03, ...,\n",
       "                 7.4268319e-06,  2.1221046e-03, -6.8786996e-03],\n",
       "               [-7.1025528e-03, -3.2592793e-03,  1.9272303e-03, ...,\n",
       "                 2.1858787e-04,  5.1001064e-03,  6.1489507e-03],\n",
       "               [ 8.5898506e-04, -1.0238646e-03, -9.9556777e-04, ...,\n",
       "                 1.6042402e-03, -2.2109835e-03,  7.6350360e-04]]],\n",
       "    \n",
       "    \n",
       "             [[[ 1.4788359e-03, -6.7594601e-03, -6.2027914e-03, ...,\n",
       "                 2.4837321e-03,  6.6028973e-03,  5.7496536e-03],\n",
       "               [ 2.0260052e-03,  3.1988020e-03,  1.5934409e-03, ...,\n",
       "                -1.5330841e-03, -1.2000976e-04,  6.6268463e-03],\n",
       "               [ 5.1961448e-03, -6.2240097e-03,  3.4983526e-03, ...,\n",
       "                 3.5549586e-03,  2.3062499e-03, -3.2821414e-03],\n",
       "               ...,\n",
       "               [ 3.1012017e-04,  6.1706491e-03,  1.1665294e-03, ...,\n",
       "                 6.2237177e-03,  4.2029484e-03, -6.8063308e-03],\n",
       "               [-1.7928076e-05,  6.6350973e-03, -1.8641707e-03, ...,\n",
       "                -4.2596897e-03,  3.0957719e-03,  3.1581055e-03],\n",
       "               [ 1.2446826e-03, -3.2799463e-03, -1.6205444e-04, ...,\n",
       "                 3.5608134e-03, -6.7040506e-03,  2.3350148e-03]],\n",
       "    \n",
       "              [[-2.2264570e-04,  2.8086845e-03,  3.7138052e-03, ...,\n",
       "                 7.1552619e-03,  1.8276122e-03,  3.5906476e-03],\n",
       "               [ 7.1573691e-03,  3.0547902e-03,  2.2974922e-03, ...,\n",
       "                -7.0006181e-03,  5.0529018e-03,  1.8302639e-03],\n",
       "               [-3.8335505e-03, -5.8518262e-03,  6.9848085e-03, ...,\n",
       "                -6.2519270e-03, -4.2323815e-04,  2.7842608e-03],\n",
       "               ...,\n",
       "               [-6.1539323e-03, -6.9595375e-03,  1.9489068e-03, ...,\n",
       "                -2.3128493e-03,  1.3550043e-03,  6.3262531e-04],\n",
       "               [ 2.8479137e-03, -6.3809752e-04,  1.5342727e-03, ...,\n",
       "                -6.6187358e-03, -6.6779396e-03, -2.4805483e-03],\n",
       "               [-3.4079556e-03, -7.0485179e-03,  6.8970597e-03, ...,\n",
       "                -6.8260455e-03, -8.6369197e-04,  4.7398400e-03]],\n",
       "    \n",
       "              [[ 4.8170099e-04,  7.1093566e-03, -2.7038096e-03, ...,\n",
       "                -2.4402174e-03,  3.0045158e-03,  3.1259607e-03],\n",
       "               [-1.5464559e-03,  2.3496337e-05,  7.3367963e-05, ...,\n",
       "                -3.0153964e-03,  5.5794339e-03,  4.6753432e-03],\n",
       "               [ 2.5013299e-04, -1.3329965e-03,  1.1950373e-03, ...,\n",
       "                -3.2200129e-03,  1.5514728e-04,  8.6061214e-04],\n",
       "               ...,\n",
       "               [-7.1115140e-03,  4.9900659e-03, -5.5911615e-03, ...,\n",
       "                 1.8480443e-03,  7.1505494e-03, -2.1389625e-03],\n",
       "               [ 5.3989151e-03, -8.7817502e-04, -1.5496972e-03, ...,\n",
       "                -1.9831536e-03, -7.1692122e-03, -5.3958325e-03],\n",
       "               [ 1.7791901e-03,  7.1262098e-03, -4.9810181e-04, ...,\n",
       "                -3.5399995e-03,  2.5989073e-03,  6.0587786e-03]]],\n",
       "    \n",
       "    \n",
       "             [[[ 3.1270098e-03, -3.2030852e-03,  6.7512100e-03, ...,\n",
       "                 6.7571038e-03, -2.6207143e-03,  7.1574654e-03],\n",
       "               [ 3.5843577e-03, -3.2853654e-03, -2.1314293e-03, ...,\n",
       "                -8.4135099e-05,  6.7067607e-03,  3.1133867e-03],\n",
       "               [-5.3774565e-04,  6.6222618e-03,  5.7616672e-03, ...,\n",
       "                -3.1068905e-03,  7.1175206e-03,  5.6524575e-03],\n",
       "               ...,\n",
       "               [-4.9423361e-03,  8.1640063e-04, -8.3426107e-04, ...,\n",
       "                -2.4689522e-03, -3.2508408e-04,  7.1918233e-03],\n",
       "               [-1.1626377e-03,  2.8871640e-03,  3.9635377e-04, ...,\n",
       "                 1.7121254e-03,  4.0135556e-04,  6.0020168e-03],\n",
       "               [-2.6101400e-03,  6.6530937e-03, -2.2119398e-03, ...,\n",
       "                 5.2581802e-03,  5.2431864e-03,  4.9101189e-03]],\n",
       "    \n",
       "              [[ 1.6082413e-03,  8.1265345e-05,  3.1971070e-04, ...,\n",
       "                -4.2116889e-03,  9.3753380e-04,  3.2618141e-03],\n",
       "               [-1.5677040e-03, -5.4589170e-03,  6.6256681e-03, ...,\n",
       "                -2.4680858e-03, -6.3383663e-03, -8.9449226e-04],\n",
       "               [-2.5105439e-03,  5.4950165e-03, -4.7269464e-04, ...,\n",
       "                -2.0317212e-03,  1.2108444e-03, -7.8167743e-04],\n",
       "               ...,\n",
       "               [ 2.8044772e-03, -2.2328920e-03,  1.5454990e-03, ...,\n",
       "                -6.1373059e-03, -7.1717449e-03, -2.7589966e-05],\n",
       "               [-1.9196231e-03, -2.5203740e-03,  4.8729591e-05, ...,\n",
       "                -2.4811295e-03, -2.9439640e-03,  6.1617722e-03],\n",
       "               [-7.4228812e-03,  1.6921990e-03, -1.3979197e-03, ...,\n",
       "                -5.7284273e-03, -5.3911675e-03,  2.5479570e-03]],\n",
       "    \n",
       "              [[-6.1607729e-03,  5.4206578e-03,  2.4578951e-03, ...,\n",
       "                 7.9688709e-04,  1.1672745e-03,  6.6282689e-03],\n",
       "               [ 1.2020913e-03, -2.3116025e-03, -1.9986080e-03, ...,\n",
       "                -4.0171738e-03,  6.8776538e-03,  2.4922045e-03],\n",
       "               [-6.3618906e-03, -6.6278614e-03, -7.1493597e-03, ...,\n",
       "                 1.9536729e-03,  2.4337601e-03, -2.9802541e-03],\n",
       "               ...,\n",
       "               [-7.1694641e-03, -6.8477564e-04,  6.9725085e-03, ...,\n",
       "                -3.0742423e-03,  1.9914710e-03,  2.4990842e-03],\n",
       "               [ 2.6559436e-03,  2.3948886e-03, -2.9641152e-03, ...,\n",
       "                -3.2699420e-03,  2.9609185e-03, -3.0194577e-03],\n",
       "               [-5.4860977e-03, -6.6184611e-03,  2.3446484e-03, ...,\n",
       "                 6.8813358e-03,  3.7340214e-03,  4.2998465e-03]]]],\n",
       "    \n",
       "    \n",
       "    \n",
       "            [[[[-2.4747087e-03,  3.2835149e-03,  5.5959350e-03, ...,\n",
       "                 7.1633025e-03,  6.3672690e-03,  2.7601151e-03],\n",
       "               [-3.8429750e-03,  6.4895619e-03,  6.8725250e-04, ...,\n",
       "                -2.4378360e-03,  1.9542133e-03, -2.1264395e-03],\n",
       "               [-3.0517632e-03,  2.1946870e-03,  2.0760894e-03, ...,\n",
       "                -4.6644830e-03, -6.5434526e-04,  6.6185119e-03],\n",
       "               ...,\n",
       "               [ 1.5442676e-03,  6.7999563e-03,  1.0773402e-03, ...,\n",
       "                 1.5265064e-03,  2.4080132e-03,  1.9979239e-03],\n",
       "               [ 5.9022354e-03, -5.7591277e-04, -4.4386205e-04, ...,\n",
       "                -7.6640160e-03, -1.5586624e-03,  2.4048446e-03],\n",
       "               [-5.6537339e-03, -1.3616006e-03, -4.7771400e-03, ...,\n",
       "                 1.7859787e-03, -7.1272585e-03,  2.3960159e-03]],\n",
       "    \n",
       "              [[ 6.0675084e-04,  2.8819200e-03,  4.8605395e-03, ...,\n",
       "                -2.2791559e-03, -1.5065656e-03, -6.7509697e-03],\n",
       "               [-2.6977668e-03, -5.4235263e-03,  2.3007439e-04, ...,\n",
       "                 1.3120646e-03, -4.6037594e-03,  1.3771721e-03],\n",
       "               [ 1.7126997e-03, -7.1720192e-03,  8.7375240e-04, ...,\n",
       "                -7.1052006e-03, -7.1304603e-03, -1.1354764e-03],\n",
       "               ...,\n",
       "               [ 7.0849145e-03, -2.4828822e-03,  3.1305996e-03, ...,\n",
       "                 1.6291814e-03,  7.0163561e-04,  8.4064732e-04],\n",
       "               [ 2.2631690e-03, -1.1668573e-03, -1.1483632e-03, ...,\n",
       "                -2.7040141e-03, -6.1458079e-03,  7.9009915e-04],\n",
       "               [-4.0228624e-04, -2.6521306e-03, -7.7544176e-04, ...,\n",
       "                -6.6636540e-03,  1.2557907e-03, -5.1996754e-03]],\n",
       "    \n",
       "              [[ 3.6639944e-03, -3.3886351e-03,  2.5561901e-03, ...,\n",
       "                -7.7239756e-04,  5.7892012e-03,  1.6495874e-03],\n",
       "               [ 1.4946886e-03, -3.6531407e-04,  2.9049753e-03, ...,\n",
       "                -1.7827738e-03, -6.7403400e-03, -1.4600235e-03],\n",
       "               [-2.1766894e-04, -6.4839837e-03, -3.6291447e-03, ...,\n",
       "                 1.9590836e-03, -3.8586466e-03,  1.4126833e-03],\n",
       "               ...,\n",
       "               [-7.0356298e-03, -7.0897755e-03, -1.7116135e-03, ...,\n",
       "                 7.4268319e-06,  2.1221046e-03, -6.8786996e-03],\n",
       "               [-7.1025528e-03, -3.2592793e-03,  1.9272303e-03, ...,\n",
       "                 2.1858787e-04,  5.1001064e-03,  6.1489507e-03],\n",
       "               [ 8.5898506e-04, -1.0238646e-03, -9.9556777e-04, ...,\n",
       "                 1.6042402e-03, -2.2109835e-03,  7.6350360e-04]]],\n",
       "    \n",
       "    \n",
       "             [[[ 1.4788359e-03, -6.7594601e-03, -6.2027914e-03, ...,\n",
       "                 2.4837321e-03,  6.6028973e-03,  5.7496536e-03],\n",
       "               [ 2.0260052e-03,  3.1988020e-03,  1.5934409e-03, ...,\n",
       "                -1.5330841e-03, -1.2000976e-04,  6.6268463e-03],\n",
       "               [ 5.1961448e-03, -6.2240097e-03,  3.4983526e-03, ...,\n",
       "                 3.5549586e-03,  2.3062499e-03, -3.2821414e-03],\n",
       "               ...,\n",
       "               [ 3.1012017e-04,  6.1706491e-03,  1.1665294e-03, ...,\n",
       "                 6.2237177e-03,  4.2029484e-03, -6.8063308e-03],\n",
       "               [-1.7928076e-05,  6.6350973e-03, -1.8641707e-03, ...,\n",
       "                -4.2596897e-03,  3.0957719e-03,  3.1581055e-03],\n",
       "               [ 1.2446826e-03, -3.2799463e-03, -1.6205444e-04, ...,\n",
       "                 3.5608134e-03, -6.7040506e-03,  2.3350148e-03]],\n",
       "    \n",
       "              [[-2.2264570e-04,  2.8086845e-03,  3.7138052e-03, ...,\n",
       "                 7.1552619e-03,  1.8276122e-03,  3.5906476e-03],\n",
       "               [ 7.1573691e-03,  3.0547902e-03,  2.2974922e-03, ...,\n",
       "                -7.0006181e-03,  5.0529018e-03,  1.8302639e-03],\n",
       "               [-3.8335505e-03, -5.8518262e-03,  6.9848085e-03, ...,\n",
       "                -6.2519270e-03, -4.2323815e-04,  2.7842608e-03],\n",
       "               ...,\n",
       "               [-6.1539323e-03, -6.9595375e-03,  1.9489068e-03, ...,\n",
       "                -2.3128493e-03,  1.3550043e-03,  6.3262531e-04],\n",
       "               [ 2.8479137e-03, -6.3809752e-04,  1.5342727e-03, ...,\n",
       "                -6.6187358e-03, -6.6779396e-03, -2.4805483e-03],\n",
       "               [-3.4079556e-03, -7.0485179e-03,  6.8970597e-03, ...,\n",
       "                -6.8260455e-03, -8.6369197e-04,  4.7398400e-03]],\n",
       "    \n",
       "              [[ 4.8170099e-04,  7.1093566e-03, -2.7038096e-03, ...,\n",
       "                -2.4402174e-03,  3.0045158e-03,  3.1259607e-03],\n",
       "               [-1.5464559e-03,  2.3496337e-05,  7.3367963e-05, ...,\n",
       "                -3.0153964e-03,  5.5794339e-03,  4.6753432e-03],\n",
       "               [ 2.5013299e-04, -1.3329965e-03,  1.1950373e-03, ...,\n",
       "                -3.2200129e-03,  1.5514728e-04,  8.6061214e-04],\n",
       "               ...,\n",
       "               [-7.1115140e-03,  4.9900659e-03, -5.5911615e-03, ...,\n",
       "                 1.8480443e-03,  7.1505494e-03, -2.1389625e-03],\n",
       "               [ 5.3989151e-03, -8.7817502e-04, -1.5496972e-03, ...,\n",
       "                -1.9831536e-03, -7.1692122e-03, -5.3958325e-03],\n",
       "               [ 1.7791901e-03,  7.1262098e-03, -4.9810181e-04, ...,\n",
       "                -3.5399995e-03,  2.5989073e-03,  6.0587786e-03]]],\n",
       "    \n",
       "    \n",
       "             [[[ 3.1270098e-03, -3.2030852e-03,  6.7512100e-03, ...,\n",
       "                 6.7571038e-03, -2.6207143e-03,  7.1574654e-03],\n",
       "               [ 3.5843577e-03, -3.2853654e-03, -2.1314293e-03, ...,\n",
       "                -8.4135099e-05,  6.7067607e-03,  3.1133867e-03],\n",
       "               [-5.3774565e-04,  6.6222618e-03,  5.7616672e-03, ...,\n",
       "                -3.1068905e-03,  7.1175206e-03,  5.6524575e-03],\n",
       "               ...,\n",
       "               [-4.9423361e-03,  8.1640063e-04, -8.3426107e-04, ...,\n",
       "                -2.4689522e-03, -3.2508408e-04,  7.1918233e-03],\n",
       "               [-1.1626377e-03,  2.8871640e-03,  3.9635377e-04, ...,\n",
       "                 1.7121254e-03,  4.0135556e-04,  6.0020168e-03],\n",
       "               [-2.6101400e-03,  6.6530937e-03, -2.2119398e-03, ...,\n",
       "                 5.2581802e-03,  5.2431864e-03,  4.9101189e-03]],\n",
       "    \n",
       "              [[ 1.6082413e-03,  8.1265345e-05,  3.1971070e-04, ...,\n",
       "                -4.2116889e-03,  9.3753380e-04,  3.2618141e-03],\n",
       "               [-1.5677040e-03, -5.4589170e-03,  6.6256681e-03, ...,\n",
       "                -2.4680858e-03, -6.3383663e-03, -8.9449226e-04],\n",
       "               [-2.5105439e-03,  5.4950165e-03, -4.7269464e-04, ...,\n",
       "                -2.0317212e-03,  1.2108444e-03, -7.8167743e-04],\n",
       "               ...,\n",
       "               [ 2.8044772e-03, -2.2328920e-03,  1.5454990e-03, ...,\n",
       "                -6.1373059e-03, -7.1717449e-03, -2.7589966e-05],\n",
       "               [-1.9196231e-03, -2.5203740e-03,  4.8729591e-05, ...,\n",
       "                -2.4811295e-03, -2.9439640e-03,  6.1617722e-03],\n",
       "               [-7.4228812e-03,  1.6921990e-03, -1.3979197e-03, ...,\n",
       "                -5.7284273e-03, -5.3911675e-03,  2.5479570e-03]],\n",
       "    \n",
       "              [[-6.1607729e-03,  5.4206578e-03,  2.4578951e-03, ...,\n",
       "                 7.9688709e-04,  1.1672745e-03,  6.6282689e-03],\n",
       "               [ 1.2020913e-03, -2.3116025e-03, -1.9986080e-03, ...,\n",
       "                -4.0171738e-03,  6.8776538e-03,  2.4922045e-03],\n",
       "               [-6.3618906e-03, -6.6278614e-03, -7.1493597e-03, ...,\n",
       "                 1.9536729e-03,  2.4337601e-03, -2.9802541e-03],\n",
       "               ...,\n",
       "               [-7.1694641e-03, -6.8477564e-04,  6.9725085e-03, ...,\n",
       "                -3.0742423e-03,  1.9914710e-03,  2.4990842e-03],\n",
       "               [ 2.6559436e-03,  2.3948886e-03, -2.9641152e-03, ...,\n",
       "                -3.2699420e-03,  2.9609185e-03, -3.0194577e-03],\n",
       "               [-5.4860977e-03, -6.6184611e-03,  2.3446484e-03, ...,\n",
       "                 6.8813358e-03,  3.7340214e-03,  4.2998465e-03]]]]]],      dtype=float32)}}}}"
      ]
     },
     "execution_count": 487,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[0.00989669, 0.        , 0.        , 0.02495345, 0.        ,\n",
       "        0.04678731, 0.        , 0.        , 0.02911181, 0.        ,\n",
       "        0.0302304 , 0.        , 0.        , 0.05054726, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.04268081, 0.        , 0.        , 0.04878178, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.0427697 , 0.        , 0.        , 0.05188805, 0.        ,\n",
       "        0.04020147, 0.        , 0.        , 0.03869548, 0.        ,\n",
       "        0.03906661, 0.        , 0.        , 0.02635786, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.0448165 , 0.        , 0.        , 0.047762  , 0.        ,\n",
       "        0.04205732, 0.        , 0.        , 0.06013509, 0.        ,\n",
       "        0.02391879, 0.        , 0.        , 0.04875704, 0.        ,\n",
       "        0.03548217, 0.        , 0.        , 0.03811829, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.08358971, 0.        , 0.        , 0.05339443, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ]],      dtype=float32)"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_STEPS = 100_000\n",
    "\n",
    "# Flashbax buffer\n",
    "max_length = NUM_STEPS\n",
    "min_length = 1\n",
    "sample_batch_size = 1\n",
    "\n",
    "add_sequences = False\n",
    "add_batch_size = None\n",
    "\n",
    "# Instantiate the flat buffer, which is a Dataclass of pure functions.\n",
    "buffer = fbx.make_item_buffer(max_length, min_length, sample_batch_size, False, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 125)\n",
      "(16, 1, 5, 5, 3)\n"
     ]
    }
   ],
   "source": [
    "# Approach 1\n",
    "init_logits = batched_get_action_and_logits(replicate_params, timesteps.observation)[1]\n",
    "buffer_state = buffer.init((timesteps.observation, init_logits))\n",
    "\n",
    "# Generate D with teacher\n",
    "def env_step(carry, _):\n",
    "    \"\"\"Step the environment.\"\"\"\n",
    "    # SELECT ACTION\n",
    "    key, env_state, last_timestep, buffer_state = carry\n",
    "    key, policy_key = jax.random.split(key)\n",
    "    action, logits = batched_get_action_and_logits(replicate_params, last_timestep.observation)\n",
    "\n",
    "    # STEP ENVIRONMENT\n",
    "    env_state, timestep = jax.pmap(jax.vmap(jax.vmap(env.step)))(env_state, action)\n",
    "\n",
    "    transition = (\n",
    "        last_timestep.observation, logits\n",
    "    )\n",
    "    buffer.add(buffer_state, transition)\n",
    "    carry = key, env_state, timestep, buffer_state\n",
    "    return carry, _\n",
    "\n",
    "keys = jax.random.split(key, )\n",
    "step_init = key, env_states, timesteps, buffer_state\n",
    "jax.lax.scan(env_step, step_init, None, NUM_STEPS);\n",
    "\n",
    "#for i in range(max_length):\n",
    "#    env_states, timesteps, buffer_state = env_step(key, env_states, timesteps, buffer_state)\n",
    "\n",
    "#env_step(key, env_states, timesteps, buffer_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def learner_setup(\n",
    "    env: Environment, keys: chex.Array, config: DictConfig\n",
    "):\n",
    "    \"\"\"Initialise learner_fn, network, optimiser, environment and states.\"\"\"\n",
    "    # Get available TPU cores.\n",
    "    n_devices = len(jax.devices())\n",
    "\n",
    "    # Get number of agents.\n",
    "    # config.system.num_agents = env.num_agents\n",
    "    config.system.num_agents = 1 \n",
    "\n",
    "    # PRNG keys.\n",
    "    key, actor_net_key = keys\n",
    "\n",
    "    # Define network and optimiser.\n",
    "    actor_torso = hydra.utils.instantiate(config.network.actor_network.pre_torso)\n",
    "    actor_action_head = hydra.utils.instantiate(\n",
    "        config.network.action_head, action_dim=env.action_dim\n",
    "    )\n",
    "\n",
    "    network = Actor(torso=actor_torso, action_head=actor_action_head)\n",
    "\n",
    "    actor_lr = make_learning_rate(config.system.actor_lr, config)\n",
    "\n",
    "    optim = optax.chain(\n",
    "        optax.clip_by_global_norm(config.system.max_grad_norm),\n",
    "        optax.adam(actor_lr, eps=1e-5),\n",
    "    )\n",
    "\n",
    "    # Initialise observation with obs of all agents.\n",
    "    obs = env.observation_spec().generate_value()\n",
    "    init_x = jax.tree_util.tree_map(lambda x: x[jnp.newaxis, ...], obs)\n",
    "\n",
    "    # Initialise actor params and optimiser state.\n",
    "    params = network.init(actor_net_key, init_x)\n",
    "    opt_state = optim.init(params)\n",
    "\n",
    "    # Pack apply and update functions.\n",
    "    apply_fn = network.apply\n",
    "    update_fn = optim.update\n",
    "\n",
    "    # Get batched iterated update and replicate it to pmap it over cores.\n",
    "    learn = get_learner_fn(env, apply_fn, update_fn, config)\n",
    "    learn = jax.pmap(learn, in_axes=(0, 0), axis_name=\"device\")\n",
    "\n",
    "    '''\n",
    "    # Load model from checkpoint if specified.\n",
    "    if config.logger.checkpointing.load_model:\n",
    "        loaded_checkpoint = Checkpointer(\n",
    "            model_name=config.logger.system_name,\n",
    "            **config.logger.checkpointing.load_args,  # Other checkpoint args\n",
    "        )\n",
    "        # Restore the learner state from the checkpoint\n",
    "        restored_params, _ = loaded_checkpoint.restore_params(input_params=params)\n",
    "        # Update the params\n",
    "        params = restored_params\n",
    "    '''\n",
    "    # Define params to be replicated across devices and batches.\n",
    "    key, step_keys = jax.random.split(key)  # noqa: E999\n",
    "    replicate_learner = (params, opt_state, step_keys)\n",
    "\n",
    "    # Duplicate learner for update_batch_size.\n",
    "    broadcast = lambda x: jnp.broadcast_to(x, (config.system.update_batch_size,) + x.shape)\n",
    "    replicate_learner = jax.tree_map(broadcast, replicate_learner)\n",
    "\n",
    "    # Duplicate learner across devices.\n",
    "    replicate_learner = flax.jax_utils.replicate(replicate_learner, devices=jax.devices())\n",
    "\n",
    "    # Initialise learner state.\n",
    "    params, opt_state, step_keys = replicate_learner\n",
    "    init_learner_state = (params, opt_state, step_keys)\n",
    "\n",
    "    return learn, network, init_learner_state\n",
    "\n",
    "def get_learner_fn(\n",
    "    env: Environment,\n",
    "    apply_fn,\n",
    "    update_fn: optax.TransformUpdateFn,\n",
    "    config: DictConfig,\n",
    ") -> LearnerFn[LearnerState]:\n",
    "    \"\"\"Get the learner function.\"\"\"\n",
    "\n",
    "\n",
    "    def _update_step(learner_state: Any, batch: Any) -> Tuple[LearnerState, Tuple]:\n",
    "\n",
    "        # Sample experience batch\n",
    "        params, opt_state, key = learner_state\n",
    "        \n",
    "        #print(batch.experience[1])\n",
    "        obs, target = batch.experience\n",
    "\n",
    "        '''\n",
    "        learner_state, traj_batch = jax.lax.scan(\n",
    "            _env_step, learner_state, None, config.system.rollout_length\n",
    "        )\n",
    "        '''\n",
    "\n",
    "        def _update_epoch(update_state: Tuple, _: Any) -> Tuple:\n",
    "            \"\"\"Update the network for a single epoch.\"\"\"\n",
    "\n",
    "            def _update_minibatch(train_state: Tuple, batch_info: Tuple) -> Tuple:\n",
    "                \"\"\"Update the network for a single minibatch.\"\"\"\n",
    "\n",
    "                # UNPACK TRAIN STATE AND BATCH INFO\n",
    "                params, opt_state, key = train_state\n",
    "                obs, target = batch_info\n",
    "\n",
    "                '''\n",
    "                def _loss_fn(\n",
    "                    params: FrozenDict,\n",
    "                    actor_opt_state: OptState,\n",
    "                    obs,\n",
    "                    outputs: chex.Array,\n",
    "                    key: chex.PRNGKey,\n",
    "                ) -> Tuple:\n",
    "                    \"\"\"Calculate the actor loss.\"\"\"\n",
    "                    # RERUN NETWORK\n",
    "                    policy = apply_fn(params, obs)\n",
    "\n",
    "                    # CALCULATE ACTOR LOSS\n",
    "                    loss_actor = optax.softmax_cross_entropy(outputs)\n",
    "                    # The seed will be used in the TanhTransformedDistribution:\n",
    "                    entropy = policy.entropy(seed=key).mean()\n",
    "                    total_loss_actor = loss_actor - config.system.ent_coef * entropy\n",
    "                    return total_loss_actor, (loss_actor, entropy)\n",
    "                '''\n",
    "\n",
    "                # CALCULATE ACTOR LOSS\n",
    "                key, entropy_key = jax.random.split(key)\n",
    "                '''\n",
    "                actor_grad_fn = jax.value_and_grad(_actor_loss_fn, has_aux=True)\n",
    "                \n",
    "                actor_loss_info, actor_grads = actor_grad_fn(\n",
    "                    params.actor_params,\n",
    "                    opt_states.actor_opt_state,\n",
    "                    traj_batch,\n",
    "                    advantages,\n",
    "                    entropy_key,\n",
    "                )\n",
    "                '''\n",
    "\n",
    "                def get_action_and_logits(params, obs):\n",
    "                    #print(params[\"params\"][\"action_head\"][\"Dense_0\"][\"kernel\"].shape)\n",
    "                    #print(obs.agents_view.shape)\n",
    "                    output = apply_fn(params, obs)\n",
    "                    logits = output.distribution.logits\n",
    "                    action = output.sample(seed=key)\n",
    "                    return action, logits\n",
    "                #batched_get_action_and_logits = jax.pmap(jax.vmap(get_action_and_logits))\n",
    "\n",
    "                #output = apply_fn(params, obs)\n",
    "                #print(params[\"params\"][\"action_head\"][\"Dense_0\"][\"kernel\"].shape)\n",
    "                #print(obs.agents_view.shape)\n",
    "\n",
    "                def loss_fn(params, obs, target):\n",
    "                    action, logits = get_action_and_logits(params, obs)\n",
    "                    return optax.softmax_cross_entropy(logits, target).mean()\n",
    "                \n",
    "                loss_info, grads = jax.value_and_grad(loss_fn)(params, obs, target)\n",
    "\n",
    "\n",
    "                # Compute the parallel mean (pmean) over the batch.\n",
    "                # This calculation is inspired by the Anakin architecture demo notebook.\n",
    "                # available at https://tinyurl.com/26tdzs5x\n",
    "                # This pmean could be a regular mean as the batch axis is on the same device.\n",
    "                grads, loss_info = jax.lax.pmean(\n",
    "                    (grads, loss_info), axis_name=\"batch\"\n",
    "                )\n",
    "                # pmean over devices.\n",
    "                grads, loss_info = jax.lax.pmean(\n",
    "                    (grads, loss_info), axis_name=\"device\"\n",
    "                )\n",
    "\n",
    "                # UPDATE ACTOR PARAMS AND OPTIMISER STATE\n",
    "                updates, new_opt_state = update_fn(\n",
    "                    grads, opt_state\n",
    "                )\n",
    "                #print(params[\"params\"][\"action_head\"][\"Dense_0\"][\"kernel\"].shape)\n",
    "                #print(updates[\"params\"][\"action_head\"][\"Dense_0\"][\"kernel\"].shape)\n",
    "                new_params = optax.apply_updates(params, updates)\n",
    "                return (new_params, new_opt_state, entropy_key), loss_info\n",
    "\n",
    "            params, opt_state, obs, target, key = update_state\n",
    "            #key, shuffle_key, entropy_key = jax.random.split(key, 3)\n",
    "\n",
    "            # (don't) SHUFFLE MINIBATCHES\n",
    "            batch = (obs, target)\n",
    "            \n",
    "            '''\n",
    "            batch = jax.tree_util.tree_map(lambda x: merge_leading_dims(x, 2), batch)\n",
    "            minibatches = jax.tree_util.tree_map(\n",
    "                lambda x: jnp.reshape(x, [config.system.num_minibatches, -1] + list(x.shape[1:])),\n",
    "                batch,\n",
    "            )\n",
    "            '''\n",
    "            # UPDATE MINIBATCHES\n",
    "            key, entropy_key = jax.random.split(key)\n",
    "            #(params, opt_state, entropy_key), loss_info = jax.lax.scan(\n",
    "            #    _update_minibatch, (params, opt_state, entropy_key), batch #minibatches\n",
    "            #)\n",
    "            (params, opt_state, entropy_key), loss_info = _update_minibatch((params, opt_state, entropy_key), batch)\n",
    "            update_state = (params, opt_state, obs, target, key)\n",
    "            return update_state, loss_info\n",
    "        \n",
    "        update_state = (params, opt_state, obs, target, key)\n",
    "\n",
    "        # UPDATE EPOCHS\n",
    "        update_state, loss_info = jax.lax.scan(\n",
    "            _update_epoch, update_state, None, config.system.ppo_epochs\n",
    "        )\n",
    "\n",
    "        params, opt_state, obs, target, key = update_state\n",
    "        learner_state =(params, opt_state, key)\n",
    "        #metric = traj_batch.info #??\n",
    "        return (learner_state, batch), (loss_info)\n",
    "        #return learner_state, (metric, loss_info)\n",
    "    \n",
    "    def learner_fn(learner_state: Any, batch: Any) -> Tuple:\n",
    "            \"\"\"Learner function.\n",
    "\n",
    "            This function represents the learner, it updates the network parameters\n",
    "            by iteratively applying the `_update_step` function for a fixed number of\n",
    "            updates. The `_update_step` function is vectorized over a batch of inputs.\n",
    "\n",
    "            Args:\n",
    "                learner_state (NamedTuple):\n",
    "                    - params (Params): The initial model parameters.\n",
    "                    - opt_states (OptStates): The initial optimizer state.\n",
    "                    - key (chex.PRNGKey): The random number generator state.\n",
    "                    - env_state (LogEnvState): The environment state.\n",
    "                    - timesteps (TimeStep): The initial timestep in the initial trajectory.\n",
    "            \"\"\"\n",
    "\n",
    "            batched_update_step = jax.vmap(_update_step, in_axes=(0, 0), axis_name=\"batch\")\n",
    "\n",
    "            def scanned_batched_update_step(carry, _):\n",
    "                return batched_update_step(carry[0], carry[1])\n",
    "   \n",
    "            (learner_state, batch), (loss_info) = jax.lax.scan(\n",
    "                scanned_batched_update_step, (learner_state, batch), None, config.system.num_updates_per_eval\n",
    "            )\n",
    "\n",
    "            print(\"yippee\")\n",
    "            return (\n",
    "                learner_state,\n",
    "                #episode_metrics=episode_info,\n",
    "                loss_info,\n",
    "            )\n",
    "\n",
    "    return learner_fn\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def run_experiment(_config: DictConfig, buffer_state: Any) -> float:\n",
    "    \"\"\"Runs experiment.\"\"\"\n",
    "    config = copy.deepcopy(_config)\n",
    "\n",
    "    n_devices = len(jax.devices())\n",
    "\n",
    "    # Create the enviroments for train and eval.\n",
    "    env, eval_env = environments.make(config)\n",
    "\n",
    "    # PRNG keys.\n",
    "    key, key_e, actor_net_key = jax.random.split(\n",
    "        jax.random.PRNGKey(config.system.seed), num=3\n",
    "    )\n",
    "\n",
    "    # Setup learner.\n",
    "    learn, actor_network, learner_state = learner_setup(\n",
    "        env, (key, actor_net_key), config\n",
    "    )\n",
    "\n",
    "    # Setup evaluator.\n",
    "    # One key per device for evaluation.\n",
    "    eval_keys = jax.random.split(key_e, n_devices)\n",
    "    evaluator, absolute_metric_evaluator = make_eval_fns(eval_env, actor_network, config)\n",
    "\n",
    "    # Calculate total timesteps.\n",
    "    config = check_total_timesteps(config)\n",
    "    assert (\n",
    "        config.system.num_updates > config.arch.num_evaluation\n",
    "    ), \"Number of updates per evaluation must be less than total number of updates.\"\n",
    "\n",
    "    # Calculate number of updates per evaluation.\n",
    "    config.system.num_updates_per_eval = config.system.num_updates // config.arch.num_evaluation\n",
    "    steps_per_rollout = (\n",
    "        n_devices\n",
    "        * config.system.num_updates_per_eval\n",
    "        * config.system.rollout_length\n",
    "        * config.system.update_batch_size\n",
    "        * config.arch.num_envs\n",
    "    )\n",
    "\n",
    "    # Logger setup\n",
    "    logger = MavaLogger(config)\n",
    "    #cfg: Dict = OmegaConf.to_container(config, resolve=True)\n",
    "    #cfg[\"arch\"][\"devices\"] = jax.devices()\n",
    "    #pprint(cfg)\n",
    "\n",
    "    # Set up checkpointer\n",
    "    '''\n",
    "    save_checkpoint = config.logger.checkpointing.save_model\n",
    "    if save_checkpoint:\n",
    "        checkpointer = Checkpointer(\n",
    "            metadata=config,  # Save all config as metadata in the checkpoint\n",
    "            model_name=config.logger.system_name,\n",
    "            **config.logger.checkpointing.save_args,  # Checkpoint args\n",
    "        )\n",
    "    '''\n",
    "\n",
    "    # Run experiment for a total number of evaluations.\n",
    "    max_episode_return = -jnp.inf\n",
    "    best_params = None\n",
    "    for eval_step in range(config.arch.num_evaluation):\n",
    "        # Train.\n",
    "        start_time = time.time()\n",
    "        key, batch_key = jax.random.split(key)\n",
    "        batch = buffer.sample(buffer_state, batch_key)\n",
    "\n",
    "        def inner(x):\n",
    "            return x[0]\n",
    "        batch = jax.tree_map(inner, batch)\n",
    "\n",
    "        params, opt_state, keys = learner_state\n",
    "        learner_output = learn(learner_state, batch)\n",
    "        \n",
    "        jax.block_until_ready(learner_output)\n",
    "\n",
    "        # Log the results of the training.\n",
    "        elapsed_time = time.time() - start_time\n",
    "        t = int(steps_per_rollout * (eval_step + 1))\n",
    "        #episode_metrics, ep_completed = get_final_step_metrics(learner_output.episode_metrics)\n",
    "        #episode_metrics[\"steps_per_second\"] = steps_per_rollout / elapsed_time\n",
    "\n",
    "        # Separately log timesteps, actoring metrics and training metrics.\n",
    "        logger.log({\"timestep\": t}, t, eval_step, LogEvent.MISC)\n",
    "        #if ep_completed:  # only log episode metrics if an episode was completed in the rollout.\n",
    "        #    logger.log(episode_metrics, t, eval_step, LogEvent.ACT)\n",
    "        logger.log(learner_output[1], t, eval_step, LogEvent.TRAIN)\n",
    "\n",
    "        # Prepare for evaluation.\n",
    "        start_time = time.time()\n",
    "        \n",
    "        trained_params = unreplicate_batch_dim(params)\n",
    "        key_e, *eval_keys = jax.random.split(key_e, n_devices + 1)\n",
    "        eval_keys = jnp.stack(eval_keys)\n",
    "        eval_keys = eval_keys.reshape(n_devices, -1)\n",
    "\n",
    "        # Evaluate.\n",
    "        print(params['params']['action_head']['Dense_0']['kernel'].shape)\n",
    "        kek\n",
    "        evaluator_output = evaluator(trained_params, eval_keys)\n",
    "        jax.block_until_ready(evaluator_output)\n",
    "\n",
    "        # Log the results of the evaluation.\n",
    "        elapsed_time = time.time() - start_time\n",
    "        episode_return = jnp.mean(evaluator_output.episode_metrics[\"episode_return\"])\n",
    "\n",
    "        steps_per_eval = int(jnp.sum(evaluator_output.episode_metrics[\"episode_length\"]))\n",
    "        evaluator_output.episode_metrics[\"steps_per_second\"] = steps_per_eval / elapsed_time\n",
    "        logger.log(evaluator_output.episode_metrics, t, eval_step, LogEvent.EVAL)\n",
    "\n",
    "        '''\n",
    "        if save_checkpoint:\n",
    "            # Save checkpoint of learner state\n",
    "            checkpointer.save(\n",
    "                timestep=steps_per_rollout * (eval_step + 1),\n",
    "                unreplicated_learner_state=unreplicate_n_dims(learner_output.learner_state),\n",
    "                episode_return=episode_return,\n",
    "            )\n",
    "        '''\n",
    "\n",
    "        if config.arch.absolute_metric and max_episode_return <= episode_return:\n",
    "            best_params = copy.deepcopy(trained_params)\n",
    "            max_episode_return = episode_return\n",
    "\n",
    "        # Update runner state to continue training.\n",
    "        learner_state, loss_info = learner_output\n",
    "\n",
    "    # Record the performance for the final evaluation run.\n",
    "    eval_performance = float(jnp.mean(evaluator_output.episode_metrics[config.env.eval_metric]))\n",
    "\n",
    "    # Measure absolute metric.\n",
    "    if config.arch.absolute_metric:\n",
    "        start_time = time.time()\n",
    "\n",
    "        key_e, *eval_keys = jax.random.split(key_e, n_devices + 1)\n",
    "        eval_keys = jnp.stack(eval_keys)\n",
    "        eval_keys = eval_keys.reshape(n_devices, -1)\n",
    "\n",
    "        evaluator_output = absolute_metric_evaluator(best_params, eval_keys)\n",
    "        jax.block_until_ready(evaluator_output)\n",
    "\n",
    "        elapsed_time = time.time() - start_time\n",
    "        steps_per_eval = int(jnp.sum(evaluator_output.episode_metrics[\"episode_length\"]))\n",
    "        t = int(steps_per_rollout * (eval_step + 1))\n",
    "        evaluator_output.episode_metrics[\"steps_per_second\"] = steps_per_eval / elapsed_time\n",
    "        logger.log(evaluator_output.episode_metrics, t, eval_step, LogEvent.ABSOLUTE)\n",
    "\n",
    "    # Stop the logger.\n",
    "    logger.stop()\n",
    "\n",
    "    return eval_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yippee\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1m\u001b[33m\u001b[1mMISC - Timestep: 20480\u001b[0m\n",
      "\u001b[36m\u001b[1m\u001b[35m\u001b[1mTRAINER - \u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2, 800, 125)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'kek' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer_state\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[29], line 353\u001b[0m, in \u001b[0;36mrun_experiment\u001b[0;34m(_config, buffer_state)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# Evaluate.\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28mprint\u001b[39m(params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maction_head\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDense_0\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkernel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m--> 353\u001b[0m \u001b[43mkek\u001b[49m\n\u001b[1;32m    354\u001b[0m evaluator_output \u001b[38;5;241m=\u001b[39m evaluator(trained_params, eval_keys)\n\u001b[1;32m    355\u001b[0m jax\u001b[38;5;241m.\u001b[39mblock_until_ready(evaluator_output)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'kek' is not defined"
     ]
    }
   ],
   "source": [
    "run_experiment(config, buffer_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [--help] [--hydra-help] [--version]\n",
      "                             [--cfg {job,hydra,all}] [--resolve]\n",
      "                             [--package PACKAGE] [--run] [--multirun]\n",
      "                             [--shell-completion] [--config-path CONFIG_PATH]\n",
      "                             [--config-name CONFIG_NAME]\n",
      "                             [--config-dir CONFIG_DIR]\n",
      "                             [--experimental-rerun EXPERIMENTAL_RERUN]\n",
      "                             [--info [{all,config,defaults,defaults-tree,plugins,searchpath}]]\n",
      "                             [overrides ...]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --f=/home/simon/.local/share/jupyter/runtime/kernel-v2-5268sPrZ1852A0oQ.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/simon/miniconda3/envs/centralc/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# Train on D with student\n",
    "    \n",
    "@hydra.main(config_path=\"../../configs\", config_name=\"default_ff_ippo.yaml\", version_base=\"1.2\")\n",
    "def hydra_entry_point(cfg: DictConfig) -> float:\n",
    "    \"\"\"Experiment entry point.\"\"\"\n",
    "    # Allow dynamic attributes.\n",
    "    OmegaConf.set_struct(cfg, False)\n",
    "\n",
    "    # Run experiment.\n",
    "    eval_performance = run_experiment(cfg, buffer_state)\n",
    "    print(f\"{Fore.CYAN}{Style.BRIGHT}Behavior Cloning experiment completed{Style.RESET_ALL}\")\n",
    "    return eval_performance\n",
    "\n",
    "\n",
    "hydra_entry_point()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "centralc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
